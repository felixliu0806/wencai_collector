import time
import random
import json
import pandas as pd
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class StockCrawler:
    def __init__(self):
        self.options = Options()
        self.options.add_argument("--disable-blink-features=AutomationControlled")
        self.options.add_argument("--start-maximized")
        # éœ€è¦æ— å¤´å¯ä»¥å–æ¶ˆæ³¨é‡Š
        # self.options.add_argument("--headless=new")
        self.driver = None
        self.wait = None
        self.all_data = []

    def init_driver(self):
        """åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨"""
        self.driver = webdriver.Chrome(options=self.options)
        self.wait = WebDriverWait(self.driver, 20)

    def wait_for_search_button(self):
        """ç­‰å¾…æœç´¢æŒ‰é’®å˜ä¸ºå¯ç”¨"""
        for _ in range(20):
            try:
                search_button = self.driver.find_element(By.CSS_SELECTOR, 'div.search-icon')
                if 'search-disable' not in search_button.get_attribute('class'):
                    return search_button
            except:
                pass
            time.sleep(0.5)
        raise Exception("æœç´¢æŒ‰é’®å§‹ç»ˆä¸å¯ç”¨")

    def get_robot_data_from_requests(self):
        """ä»è¯·æ±‚ä¸­è·å–æ•°æ®"""
        for request in self.driver.requests:
            if request.response and 'get-robot-data' in request.url and request.response.status_code == 200:
                try:
                    import gzip
                    response_data = gzip.decompress(request.response.body).decode('utf-8')
                    json_data = json.loads(response_data)
                    # æå– datas æ•°æ®
                    datas = json_data.get('data', {}).get('answer', [])[0].get('txt', [])[0].get('content', {}).get('components', [])[0].get('data', {}).get('datas', [])
                    if datas:
                        self.all_data.extend(datas)
                        print(f"âœ” å½“å‰é¡µæŠ“å– {len(datas)} æ¡è®°å½•ï¼Œç´¯è®¡ {len(self.all_data)} æ¡")
                    break
                except Exception as e:
                    print("âŒ è§£æå¤±è´¥ï¼š", e)

    def crawl_data(self, query):
        """çˆ¬å–æ•°æ®çš„ä¸»å‡½æ•°"""
        try:
            # æ‰“å¼€ç½‘é¡µ
            self.driver.get("https://www.iwencai.com/unifiedwap/")

            # è¾“å…¥æœç´¢å†…å®¹
            search_input = self.wait.until(EC.presence_of_element_located((By.ID, 'searchInput')))
            search_input.clear()
            search_input.send_keys(query)

            # ç‚¹å‡»æœç´¢æŒ‰é’®
            search_button = self.wait_for_search_button()
            search_button.click()

            # ç­‰å¾…ç¬¬ä¸€é¡µæ•°æ®åŠ è½½å®Œæˆ
            time.sleep(3)
            self.get_robot_data_from_requests()

            # è‡ªåŠ¨ç¿»é¡µ
            page_count = 1
            while True:
                try:
                    next_btn = self.driver.find_element(By.XPATH, "//a[text()='ä¸‹é¡µ']")
                    if next_btn.get_attribute("tabindex") == "-1":
                        print(f"âœ… å·²åˆ°æœ€åä¸€é¡µï¼Œå…± {page_count} é¡µï¼Œç´¯è®¡ {len(self.all_data)} æ¡")
                        break
                    else:
                        delay = random.uniform(1, 3)
                        next_btn.click()
                        time.sleep(delay)
                        page_count += 1
                        self.get_robot_data_from_requests()
                except Exception as e:
                    print("âš ï¸ æ‰¾ä¸åˆ°ä¸‹é¡µæŒ‰é’®æˆ–å…¶ä»–é”™è¯¯ï¼š", e)
                    break

            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡ï¼š")
            print(f"   - æ€»é¡µæ•°ï¼š{page_count} é¡µ")
            print(f"   - æ€»è®°å½•æ•°ï¼š{len(self.all_data)} æ¡")
            
            # è½¬æ¢ä¸ºDataFrameå¹¶è¿”å›
            return pd.DataFrame(self.all_data)

        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            return pd.DataFrame()

        finally:
            if self.driver:
                self.driver.quit()

def crawl_stock_data(query):
    """ä¾¿æ·çš„çˆ¬å–å‡½æ•°"""
    crawler = StockCrawler()
    crawler.init_driver()
    return crawler.crawl_data(query)

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    # ç¤ºä¾‹æŸ¥è¯¢
    query = "æ‰€æœ‰Aè‚¡"
    print(f"å¼€å§‹çˆ¬å–æ•°æ®ï¼ŒæŸ¥è¯¢æ¡ä»¶ï¼š{query}")
    
    try:
        df = crawl_stock_data(query)
        if not df.empty:
            print("\næ•°æ®é¢„è§ˆï¼š")
            print(df.head())
            
            # ä¿å­˜åˆ°CSVæ–‡ä»¶
            output_file = "stock_data.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"\næ•°æ®å·²ä¿å­˜åˆ°ï¼š{output_file}")
        else:
            print("æœªè·å–åˆ°æ•°æ®")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™ï¼š{str(e)}")

if __name__ == "__main__":
    main() 